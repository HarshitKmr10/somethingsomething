{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## CODE 1\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport string\nimport re\nimport unicodedata\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score as acc\nimport joblib\n\n# Initialize LabelEncoder\nLE = LabelEncoder()\n\n# Read the Excel file into a DataFrame\ndf = pd.read_excel(\"traindf.xlsx\")\n\n# Convert 'Meeting Notes' column to string type\ndf['Meeting Notes'] = df['Meeting Notes'].astype(str)\n\n# Get unique topics from the 'Topic' column\nunique_topics = df['Topic'].unique()\nprint(\"Unique Topics:\", unique_topics)\n\n\n# Define functions to preprocess text data\ndef remove_URL(text):\n    return re.sub(r\"https?://\\S+|www.\\S+\", \"\", text)\n\ndef remove_non_ascii(text):\n    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n\ndef remove_html(text):\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    return re.sub(html, \"\", text)\n\n\n# Print initial DataFrame shape and head\nprint(\"Initial DataFrame Shape:\", df.shape)\nprint(\"Initial DataFrame Head:\", df.head)\n\n# Keep only relevant columns and drop rows with missing values\ndf = df[['Meeting Notes', \"Topic\"]]\ndf.dropna(inplace=True)  # Added inplace=True to modify the DataFrame\nprint(\"DataFrame Shape After Dropping NaNs:\", df.shape)\n\n# Apply preprocessing functions to the 'Meeting Notes' column\ndf['Clean'] = df['Meeting Notes'].apply(remove_URL)\ndf['Clean'] = df['Clean'].apply(remove_non_ascii)\ndf['Clean'] = df['Clean'].apply(remove_html)\n\n# Fit LabelEncoder to the 'Topic' column\nle = LE.fit(df[\"Topic\"])\ndf[\"Topic\"] = le.transform(df[\"Topic\"])\n\n# Separate features (x) and target labels (y)\nx = df[\"Clean\"]\ny = df[\"Topic\"]\n\n# Create a pipeline with TF-IDF vectorization and LightGBM classifier\nlgb_model = Pipeline((\n    (\"vect\", TfidfVectorizer()),\n    (\"clc\", lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass'))\n))\nlgb_model.fit(x, y)\n\n# Set threshold for prediction probabilities\nthresholding = False  # Set this to True to enable thresholding\nthreshold = 0.90 if thresholding else 0.0\n\n# Get prediction probabilities for training data\ntrain_probabilities = lgb_model.predict_proba(x)\ntopic_names = le.inverse_transform(np.arange(len(lgb_model.classes_)))\nresult_df = pd.DataFrame(train_probabilities, columns=topic_names)\n\n# Apply thresholding to predicted classes\npredicted_classes = [topic_names[i] for i in np.argmax(train_probabilities, axis=1)]\nfiltered_classes = [cls if prob > threshold else 'Other' for cls, prob in zip(predicted_classes, np.max(train_probabilities, axis=1))]\n\n# Add predicted classes and result probabilities to DataFrame\ndf[\"predictedtopic\"] = filtered_classes\ndf = pd.concat([df, result_df], axis=1)\n\n# Filter out rows with 'Other' predicted topic\ndf = df[df[\"predictedtopic\"] != \"Other\"]\n\n# Calculate training accuracy\ntrain_acc = acc(le.transform(df[\"predictedtopic\"]), df['Topic'])\n\n# Inverse transform the 'Topic' column back to its original values\ndf['Topic'] = le.inverse_transform(df['Topic'])\n\n# Rename the probability columns with topic names\nfor topic in topic_names:\n    df[f'Probability_{topic}'] = df[topic]\n\n# Remove redundant columns and export the DataFrame to an Excel file\ndf.drop(columns=topic_names, inplace=True)\ndf.to_excel(\"MFAtrainingfinal.xlsx\", index=False)\nprint(\"Train Accuracy:\", train_acc)\nprint(\"Topic Names:\", topic_names)\n\n# Save the trained model using joblib\njoblib.dump(lgb_model, \"model.pkl\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code 2 Just comments","metadata":{}},{"cell_type":"code","source":"Testing Notebook\nimportant\nFor each function comment the function what is the function intend to do\ncomments hyper parameter tuning\n\n** If i can setup a testing model just predict model wrapping the code into functions**\n\nimport pandas as pd\nimport numpy as np\nimport string\nimport re\nimport unicodedata\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score as acc\nimport lightgbm as lgb\nimport joblib\n\n# Initialize LabelEncoder\nLE = LabelEncoder()\n\n# Read the test data from Excel\ndf = pd.read_excel(\"testdf.xlsx\")\n\n# Convert 'Meeting Notes' column to string type\ndf['Meeting Notes'] = df['Meeting Notes'].astype(str)\n\n# Keep only relevant columns\ndf = df[['Meeting Notes', \"Topic\"]]\n\n# Get unique topics from the 'Topic' column\nunique_topics = df['Topic'].unique()\n\n\n# Define functions to preprocess text data\ndef remove_URL(text):\n    return re.sub(r\"https?://\\S+|www.\\S+\", \"\", text)\n\ndef remove_non_ascii(text):\n    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n\ndef remove_html(text):\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    return re.sub(html, \"\", text)\n\n# Apply preprocessing functions to the 'Meeting Notes' column\ndf['Clean'] = df['Meeting Notes'].apply(remove_URL)\ndf['Clean'] = df['Clean'].apply(remove_non_ascii)\ndf['Clean'] = df['Clean'].apply(remove_html)\n\n# Encode the 'Topic' column using LabelEncoder\ndf[\"Topic\"] = LE.fit_transform(df[\"Topic\"])\n\n# Separate features (x) and target labels (y)\nx = df[\"Clean\"]\ny = df[\"Topic\"]\n\n# Load the pre-trained LightGBM model using joblib\nlgb_model = joblib.load(\"model.pkl\")\n\n# Set threshold for prediction probabilities\nthreshold = 0.75\n\n# Get topic names and prediction probabilities for test data\ntopic_names = LE.inverse_transform(np.arange(len(lgb_model.classes_)))\nprobabilities = lgb_model.predict_proba(x)\nresult_df = pd.DataFrame(probabilities, columns=topic_names)\n\n# Apply thresholding to predicted classes\npredicted_classes = [topic_names[i] for i in np.argmax(probabilities, axis=1)]\nfiltered_classes = [cls if prob > threshold else 'NAN' for cls, prob in zip(predicted_classes, np.max(probabilities, axis=1))]\n\n# Add predicted classes and result probabilities to DataFrame\ndf[\"predictedtopic\"] = filtered_classes\ndf2 = df.copy()\ndf2 = df2[df2[\"predictedtopic\"] != \"NAN\"]\n\n# Calculate the number of records in df2\nnum_records_df2 = len(df2)\nprint(\"Number of records in df2:\", num_records_df2)\n\n# Calculate accuracy for filtered data\ntrain_acc = acc(LE.transform(df2[\"predictedtopic\"]), df2['Topic'])\nprint(\"Accuracy:\", train_acc)\n\n# Concatenate prediction probabilities with the original DataFrame\ndf = pd.concat([df, result_df], axis=1)\n\n# Inverse transform the 'Topic' column back to its original values\ndf[\"Topic\"] = LE.inverse_transform(df[\"Topic\"])\n\n# Apply thresholding to prediction probabilities and add as a new column\nfiltered_prob = [prob if prob > threshold else \"NAN\" for prob in np.max(probabilities, axis=1)]\ndf[\"probability\"] = filtered_prob\n\n# Inverse transform the encoded labels back to their original values\n\n# Export the final DataFrame to an Excel file\ndf.to_excel('final_results.xlsx', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code 3","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport unicodedata\nimport string\nimport pickle\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import pipeline\nimport gc\n\nLoad sentiment analysis model\npipe = pipeline(model=\"siebert/sentiment-roberta-large-english\")\n\ndef encoding(text):\ntext = unicodedata.normalize('NFKD', text)\nreturn text\n\ndef remove_non_ascii(text):\nreturn re.sub(r'[^\\x00-\\x7f]', r'', text)\n\ndef remove_punct(text):\nreturn text.translate(str.maketrans('', '', string.punctuation))\n\ndef truncate_text(text, max_length):\n# Truncate text to fit within max_length\nif len(text) <= max_length:\nreturn text\nelse:\nreturn text[:max_length-3] + '...'\n\ndef predict_sentiment(text, results):\n# Clean the text\ncleaned_text = remove_punct(remove_non_ascii(encoding(text))).lower()\n# Truncate text to fit within the model's maximum sequence length\ncleaned_text = truncate_text(cleaned_text, 512)\n# Predict sentiment\nsentiment = pipe(cleaned_text)\nlabel = sentiment[0]['label']\nscores = sentiment[0]['score'] # Fix here, change 'results' to 'sentiment'\nreturn label, scores\n\ndef process_row(row, results):\nlabel, scores = predict_sentiment(row['Meeting Notes'], results)\nrow['sentiment_label'] = label\nrow['sentiment_scores'] = scores\nreturn row\n\nRead the Excel file into a DataFrame\ndf = pd.read_excel(\"FinaldataCRNEW.xlsx\")\n\nprint(df.shape)\nprint(df.head())\n\nDrop rows with missing 'Meeting Notes' values\ndf.dropna(subset=['Meeting Notes'], inplace=True)\ndf.reset_index(inplace=True, drop=True)\n\nprint(df.shape)\n\nProcess one row at a time\nresults = []\nfor index, row in df.iterrows():\nprocessed_row = process_row(row, results)\nresults.append(processed_row)\n# Clear memory\ndel processed_row\ngc.collect()\n\nCreate a new DataFrame from the processed rows\ndf_processed = pd.DataFrame(results)\n\nprint(df_processed)\ndf_processed.to_csv('Totalnew.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code 2 with functions","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport joblib\n\ndef preprocess_text(text):\n    text = re.sub(r\"https?://\\S+|www.\\S+\", \"\", text)\n    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    return re.sub(html, \"\", text)\n\ndef load_test_data(file_path):\n    df = pd.read_excel(file_path)\n    df['Meeting Notes'] = df['Meeting Notes'].astype(str)\n    df = df[['Meeting Notes', \"Topic\"]]\n    return df\n\ndef apply_model_and_threshold(model, df, threshold):\n    x = df[\"Meeting Notes\"].apply(preprocess_text)\n    probabilities = model.predict_proba(x)\n    topic_names = LE.inverse_transform(np.arange(len(model.classes_)))\n    predicted_classes = [topic_names[i] for i in np.argmax(probabilities, axis=1)]\n    filtered_classes = [cls if prob > threshold else 'NAN' for cls, prob in zip(predicted_classes, np.max(probabilities, axis=1))]\n    result_df = pd.DataFrame(probabilities, columns=topic_names)\n    df[\"predictedtopic\"] = filtered_classes\n    df2 = df[df[\"predictedtopic\"] != \"NAN\"]\n    num_records_df2 = len(df2)\n    train_acc = acc(LE.transform(df2[\"predictedtopic\"]), df2['Topic'])\n    df = pd.concat([df, result_df], axis=1)\n    return df, num_records_df2, train_acc\n\ndef save_final_results(df, model, threshold):\n    df[\"Topic\"] = LE.inverse_transform(df[\"Topic\"])\n    probabilities = model.predict_proba(df[\"Meeting Notes\"].apply(preprocess_text))\n    filtered_prob = [prob if prob > threshold else \"NAN\" for prob in np.max(probabilities, axis=1)]\n    df[\"probability\"] = filtered_prob\n    df.to_excel('final_results.xlsx', index=False)\n\n# Initialize LabelEncoder\nLE = LabelEncoder()\n\n# Load test data\ntest_data = load_test_data(\"testdf.xlsx\")\n\n# Load pre-trained model\nloaded_model = joblib.load(\"model.pkl\")\n\n# Set threshold for prediction probabilities\nprediction_threshold = 0.75\n\n# Apply model and threshold, calculate results\nprocessed_data, num_records, accuracy = apply_model_and_threshold(loaded_model, test_data, prediction_threshold)\nprint(\"Number of records in df2:\", num_records)\nprint(\"Accuracy:\", accuracy)\n\n# Save final results to Excel file\nsave_final_results(processed_data, loaded_model, prediction_threshold)\n","metadata":{},"execution_count":null,"outputs":[]}]}